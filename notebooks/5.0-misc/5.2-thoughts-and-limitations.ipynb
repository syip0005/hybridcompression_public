{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9364b8",
   "metadata": {},
   "source": [
    "### Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa45124",
   "metadata": {},
   "source": [
    "* Applicability of the model only for really mass batch compression or dimensionality reduction for task;\n",
    "* Performance of model is better on related datasets (I'm assuming a more concise input distribution **we can check this**)\n",
    "\n",
    "Model\n",
    "* Location of the LSTM cell, is it better at the end or the start (start = makes more \"theoretical\" sense, end = less parameters)\n",
    "* We're performing worse than other papers including:\n",
    "    * https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8807267: they use yearly load profiles?\n",
    "    * https://www.sciencedirect.com/science/article/pii/S0142061519318502: they don't use a validation set? severely over fit.\n",
    "    * https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9449764: not sure...\n",
    "* We should look at distribution of MSEs.\n",
    "* Why are we worse than image compression which is start of the art?\n",
    "** https://arxiv.org/abs/1611.01704: results are unsurprising. A convolutional based image compression is effectively \"on-par\" with JPEG2000 which is based on DWT (nb: not 1-1 comparison due to end-to-end rate/distortion quanitization arithmetic encoding utilised)\n",
    "** https://arxiv.org/pdf/1802.01436.pdf: starts to outperform JPEG2000 but uses a hyperprior model instead of a factorized model (i.e., no explicit assumption to quanitization distribution)\n",
    "* Experiments\n",
    "    * We did experiment with activation functions (GDN, Prelu, RElu) and found no significant difference;\n",
    "    * We did also experiment with order of LSTM and found that it was poorer performance at the end.\n",
    "    * We did also experiment with GRU, LSTM, RNN and found LSTM to be the best.\n",
    "    * Bidirectionality and layers of RNN unit.\n",
    "    * Batch size etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef77d9",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3658e595",
   "metadata": {},
   "source": [
    "Datasets\n",
    "\n",
    "* Comparison of frequencies are from different datasets (should be using same dataset, but measured at different frequencies);\n",
    "* DRED is processed in a different way (test set is new dates, not other houses);\n",
    "\n",
    "High-frequency\n",
    "\n",
    "* Normalization on high-frequency datasets is a difficult idea (do we still normalize to daily, or normalize to our actual compression level, i.e., wide_freq level?;\n",
    "\n",
    "Model\n",
    "\n",
    "* Not enough thorough comparison of hyperparameters;\n",
    "* No change of convolutional settings for higher frequency datasets;\n",
    "* Certainly doesn't scale as well as the SCSAE (due to location invariance, **do model param comparison**);\n",
    "* HAE is it comparable to SCSAE is we used the same number of params? (**could test?**) ;\n",
    "* Scalability on both SCSAE and HAE are poor when scaled to larger datasets, because of the encoder layer\n",
    "\n",
    "Future works\n",
    "* Attention\n",
    "* Rule based way of choosing the best input days especially for high frequency (i.e., DRED)?\n",
    "* Quanitization (follow great work in image compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e6fef",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6285c7",
   "metadata": {},
   "source": [
    "* Illustrate DCUs and Smart Meters and Management Services so we can:\n",
    "    * Show why we need to use validation loss;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
